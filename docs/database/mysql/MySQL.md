# 基本概念

DB：DataBase数据库

DBMS：DataBase Management System数据库管理系统

DQL：Data Query Language查询语句

- select查询

DML：Data Manipulation Language数据操作语言

- 增insert、删delete、改update
- 针对表中数据

DDL：Data Definition Language数据定义语言

- 针对表结构
- create新建、drop删除、alter修改

TCL：Transaction Control Language事务控制语言

DCL：Data Control Language数据控制语言

- 授权grant、撤销权限revoke

<br>

# 一句SQL的执行过程

[执行过程](https://mp.weixin.qq.com/s?__biz=Mzg2OTA0Njk0OA==&mid=2247485097&idx=1&sn=84c89da477b1338bdf3e9fcd65514ac1&chksm=cea24962f9d5c074d8d3ff1ab04ee8f0d6486e3d015cfd783503685986485c11738ccb542ba7&token=79317275&lang=zh_CN%23rd)

SQL 等执行过程分为两类

- 对于查询等过程如下：权限校验---》查询缓存---》分析器---》优化器---》权限校验---》执行器---》引擎

- 对于更新等语句执行流程如下：分析器---》权限校验---》执行器---》引擎---redo log prepare---》binlog---》redo log commit

## 一、查询语句

```sql
select * from tb_student  A where A.age='18' and A.name=' 张三 ';
```

- 先检查该语句是否有**权限**，如果没有权限，直接返回错误信息，如果有权限再进行下一步

- 在 MySQL8.0 版本以前（之后的mysql删除了缓存功能），会先查询缓存，以这条 sql 语句为 key 在内存中查询是否有结果，如果有直接缓存，如果没有，执行下一步。

	> 对一个表更新的话，这个表上的所有的查询缓存都会被清空，缓存失效的场景挺多。

- 通过分析器进行词法分析，提取 sql 语句的关键元素，比如提取上面这个语句是查询 select，提取需要查询的表名为 tb_student，需要查询所有的列，查询条件是这个表的 id='1'。

- 通过分析器进行语法分析，判断这个 sql 语句是否有语法错误，比如关键词是否正确等等，如果检查没问题就执行下一步。

- 接下来就是优化器进行确定执行方案，上面的 sql 语句，可以有两种执行方案：

	```
	  a.先查询学生表中姓名为“张三”的学生，然后判断是否年龄是 18。
	  b.先找出学生中年龄 18 岁的学生，然后再查询姓名为“张三”的学生。
	```

	优化器根据自己的优化算法进行选择执行效率最好的一个方案（优化器认为，有时候不一定最好）。

- 进行权限校验，如果没有权限就会返回错误信息，如果有权限就会调用数据库引擎接口，返回引擎的执行结果。

## 二、更新语句

```sql
update tb_student A set A.age='19' where A.name=' 张三 ';
```

执行更新的时候要记录日志，MySQL 自带的日志模块是 **binlog（归档日志）** ，所有的存储引擎都可以使用，InnoDB 引擎还自带了一个日志模块 **redo log（重做日志）**。以 Innodb 为例，流程如下：

- 先查询到张三这一条数据，如果有缓存，也是会用到缓存。
- 然后拿到查询的语句，把 age 改为 19，并调用引擎 API 接口，写入这一行数据，InnoDB 引擎把数据保存在内存中，同时记录 redo log，此时 redo log 进入 prepare 状态，然后告诉执行器，执行完成了，随时可以提交。
- 执行器收到通知后记录 binlog，然后调用引擎接口，提交 redo log 为提交状态。
- 更新完成。

> binlog 是记录所有数据库**表结构变更**（例如CREATE、ALTER TABLE…）以及表**数据修改**（INSERT、UPDATE、DELETE…）的二进制日志。
>
> binlog 是server层的，所有引擎都可以使用，用于主从复制和数据恢复
>
> Note：redo log 是引擎层面的，只有 Innodb 才有，用于崩溃恢复

### 补充：为什么 redo log 要引入 prepare 预提交状态？

binlog是所有引擎都带的，redo log是Innodb用来支持事务的。

- **先写 redo log 直接提交，然后写 binlog**，假设写完 redo log 后，机器挂了，binlog 日志没有被写入，那么机器重启后，这台机器会通过 redo log 恢复数据，但是这个时候 bingog 并没有记录该数据，后续进行**机器备份**的时候，就会丢失这一条数据，同时**主从同步也会丢失这一条数据**。

- **先写 binlog，然后写 redo log**，假设写完了 binlog，机器异常重启了，由于没有 redo log，本机是无法恢复这一条记录的，但是 binlog 又有记录，那么和上面同样的道理，就会产生数据不一致的情况。

> 假设 redo log 处于预提交状态，binglog 也已经写完了，这个时候发生了异常重启会怎么样呢？ 这个就要依赖于 MySQL 的处理机制了，MySQL 的处理过程如下：
>
> - 判断 redo log 是否完整，如果判断是完整的，就立即提交。
> - 如果 redo log 只是预提交但不是 commit 状态，这个时候就会去判断 binlog 是否完整，如果完整就提交 redo log，不完整就回滚事务。

# 存储引擎

存储引擎就是表处理器

## 一、InnoDB

MySQL8.0之后的默认存储引擎，5.5之前是MyIASM。

**特点**

- 支持**事务**
- 支持数据库崩溃后自动恢复机制，安全
- 支持**外键约束**（仅有它支持），并支持**行级锁**，因此支持写并发
- 不存储总行
- 主键索引采用**聚簇索引**（并且索引的数据域存储数据文件本身），辅助索引（二级索引、**非聚簇索引**）的数据域存储主键值；因此从辅助索引查找数据，需要先通过辅助索引找到主键值，再**回表**访问聚簇索引。

<br>

## 二、MyIASM

**特点**

- 可以转换为压缩、只读表来节省空间
- 不支持事务，安全性低
- 支持**表级锁**，每次操作都是对整个表加锁
- 储存表的总行数
- 采用**非聚集索引**，索引文件的数据域存储指向数据文件的指针。辅索引与主索引基本一致，但是辅索引不用保证唯一性

<br>

## Note：MyISAM和InnoDB区别

- 都是B+tree索引

| InnoDB                 | MyISAM                         |
| ---------------------- | ------------------------------ |
| 聚簇索引+非聚簇索引    | 非聚簇索引                     |
| 数据和索引一起保存.ibd | 表结构.frm、索引.myi、数据.myd |
| 支持事务、外键、行表锁 | 不支持事务，支持表锁           |
| 更新快                 | 查询快                         |

- MyISAM适合读多更新少的：**索引和数据分开储存**，因此读取更快
- InnoDB适合插入更新频繁：**索引和数据放一起**。建立索引复杂，使用行锁，更新频繁效率更高

<br>

## 三、MEMORY

- 数据存储在内存中

- 行的长度固定

- 速度快

**优点**：查询效率最高，不需要和硬盘交互

**缺点**：不安全，关机后数据消失，因为数据和索引都是在内存当中



<br>

# 事务

事务是逻辑上的一组操作，要么都执行，要么都不执行。

一个事务其实就是一个完整的业务逻辑，是一个最小的工作单元。不可再分。

**事务：就是批量的DML语句同时成功，或者同时失败！**

- 提交事务？commit
	- 清空事务性活动的日志文件，**将数据全部彻底持久化到数据库表中**。
	- 提交事务标志着，事务的结束。并且是一种**全部成功**的结束。

- 回滚事务？rollback
	- 将之前所有的**DML操作全部撤销**，并且清空事务性活动的日志文件。
	- 回滚事务标志着，事务的结束。并且是一种**全部失败**的结束。
	- 回滚只能回滚至上一次提交点。

<br>

## 一、事务的四个特性（ACID）

### A：原子性

- 不可分割的最小单元
- 在同一个事务当中，所有操作必须同时成功，或者同时失败
- **undo log** 保证，事务回滚时撤销已经执行成功的sql

### C：一致性

- 一致性是指事务执行前后，数据从一个 **合法性状态** 变换到另外一个 **合法性状态** 。这种状态 是 **语义上** 的而不是语法上的，跟具体的业务有关。

- 操作前后，总量不变
- 由其他三大特性共同保证

### I：隔离性

- 多事务操作间不会相互影响
- 由**锁**机制或 **MVCC** 保证

### D：持久性

- 提交后，表中数据发生真的变化
- **redo log + 内存**保证



<br>

## 二、事务的隔离级别

1. **读未提交** - read uncommitted （最低级）
	- 没有提交就读到了
	- 事务A可以读取到事务B未提交的数据。
2. **读已提交** - read committed
	- 提交之后才能读到
	- 事务A只能读取到事务B提交之后的数据。
	- oracle默认
	- 解决了脏读现象
3. **可重复读** - repeatable read
	- 提交之后也读不到，永远读取的都是刚开启事务时的数据
	- 事务A开启之后，不管是多久，每一次在事务A中读取到的数据都是一致的。即使事务B将数据已经修改，并且提交了，事务A读取到的数据还是没有发生改变，这就是可重复读。
	- 解决了不可重复读的问题
	- **MySQL默认**
4. 序列化/串行化 - serializable （最高级）
	- 事务排队，不能并发

<br>

### 1、脏读、不可重复读、幻读

1. 脏读
	- 一个未提交事务读取到另一个未提交事务的数据
2. 不可重复读
	- 一个未提交事务读取到了另一个提交事务**修改**的数据
	- 同样的条件，你读取过的数据，再次读取出来发现值不一样了。
3. 幻读
	- 一个未提交事务读取到另一个提交事务的**添加**（增删）数据
	- 系统管理员A将数据库中所有学生的成绩从具体分数改为ABCDE等级，但是系统管理员B就在这个时候**插入（注意是插入或者删除，不是修改）**了一条具体分数的记录，当系统管理员A改结束后发现还有一条记录没有改过来，就好像发生了幻觉一样。这就叫幻读。
	- 对InnoDB不可能

#### 通过设置隔离级别，解决读问题

|          | 脏读 | 不可重复读 | 幻读 |
| -------- | ---- | ---------- | ---- |
| 读未提交 | 有   | 有         | 有   |
| 读已提交 | 无   | 有         | 有   |
| 可重复读 | 无   | 无         | 有   |
| 串行化   | 无   | 无         | 无   |

> 基本不用读未提交和串行化
>
> **MySQL 在可重复读隔离级别上也一同解决了幻读：因为使用了 MVCC**

**并发事务时还可能出现丢失修改**

丢失修改(**Lost to modify**)：指在一个事务读取一个数据时，另外一个事务也访问了该数 据，那么在第一个事务中修改了这个数据后，第二个事务也修改了这个数据。这样第一个事 务内的修改结果就被丢失，因此称为丢失修改。

例如：事务1读取某表中的数据A=20，事 务2也读取A=20，事务1修改A=A-1，事务2也修改A=A-1，最终结果A=19，事务1的修改被丢失。

### 2、如何解决脏读、幻读、不可重复读？

1. **读操作利用 MVCC，写操作进行加锁**
	- 读-写 操作彼此并不冲突， 性能更高。 
2. 读写都用加锁操作
	- 读-写 操作彼此需要排队执行 ，影响性能。



## 三、事务日志

- REDO LOG 称为**重做日志**，提供再写入操作，恢复提交事务修改的页操作，用来保证事务的**持久性**。
	- 记录的是数据页的**物理变化**，具体的数据，如页号、偏移量等
- UNDO LOG 称为**回滚日志**，回滚行记录到某个特定版本，用来保证事务的`原子性`、`一致性`。
	- 记录的是**逻辑操作**，如进行 `INSERT` 后，记录一条相反的 `DELETE` 操作。
- undo 不是 redo 的逆过程

## 四、事务的实现

Innodb通过Buffer Pool, LogBuffer, Redo Log, Undo Log来实现事务，以一个update语句为例

1. Innodb 在收到一个 update 语句后，会先根据条件找到数据所在的页，并将该页缓存在Buffer Pool中
2. 执行 update 语句，修改Buffer Pool中的数据，也就是内存中的数据
3. 针对 update 语句生成一个 RedoLog 对象，并存入LogBuffer中

4. 针对 update 语句生成undolog日志，用于事务回滚
5. 如果事务提交，那么则把RedoLog对象进行持久化，后续还有其他机制将Buffer Pool中所修改的数据页持久化到磁盘中
6. 如果事务回滚，则利用undolog日志进行回滚

<br>

# MVCC多版本并发控制

MVCC (Multiversion Concurrency Control) 是通过**数据行的多个版本管理来实现数据库的并发控制**。 

**读取数据时**通过⼀种类似**快照**的⽅式将数据保存下来，这样读锁就和写锁不冲突了，不同的事务 session 会看到⾃⼰特定版本的数据、版本链。 

> **MVCC 的实现依赖于：隐藏字段、Undo Log、Read View。**

## 隐藏字段、Undo Log版本链

- `trx_id`：每次一个事务对某条聚簇索引记录进行改动时，都会把该事务的**事务id**赋值给 **trx_id** 隐藏列。
- `roll_pointer`：每次对某条聚簇索引记录进行改动时，都会把旧的版本写入到 **undo log** 中，然后这个隐藏列就相当于一个指针，可以通过它来找到该记录修改前的信息。

<img src="https://cdn.jsdelivr.net/gh/YiENx1205/cloudimgs/notes/202207080948469.png" alt="iShot_2022-07-08_09.48.09" style="zoom:50%;" />

**MVCC 只在 READ COMMITTED 和 REPEATABLE READ 两个隔离级别下⼯作。**其他两个隔离级别和MVCC不兼容, 因为 READ UNCOMMITTED 总是读取最新的数据⾏, ⽽不是符合当前事务版本的数据⾏。⽽ SERIALIZABLE 则会对所有读取的⾏都加锁。 

<br>

## ReadView

ReadView 就是事务在使用 MVCC 机制进行快照读操作时产生的读视图

**快照**

- 快照读又叫一致性读，读取的是快照数据。
- **不加锁的简单的** **SELECT** **都属于快照读**，即不加锁的非阻塞读。

记录已提交事务所做的更改

ReadView的存在本身就保证了**事务不可以读取到未提交的事务所做的更改** ，也就是避免了脏读现象

### 四个重要属性

1. `creator_trx_id`，创建这个 Read View 的事务 ID。

> 说明：只有在对表中的记录做改动时（执行INSERT、DELETE、UPDATE这些语句时）才会为事务分配事务id，否则在一个只读事务中的事务id值都默认为0。 

2. `trx_ids`，表示在生成ReadView时当前系统中活跃的读写事务的**事务id列表**。 

3. `up_limit_id`，活跃的事务中最小的事务 ID。 

4. `low_limit_id`，表示生成ReadView时系统中**应该分配给下一个事务的id值**。low_limit_id 是系统最大的事务id值，这里要注意**是系统中的事务id**，需要区别于正在活跃的事务ID。

> 注意：low_limit_id并不是trx_ids中的最大值，事务id是递增分配的。比如，现在有id为1， 2，3这三个事务，之后id为3的事务提交了。那么一个新的读事务在生成ReadView时，trx_ids就包括1和2，up_limit_id的值就是1，low_limit_id的值就是4。

### 规则

有了这个ReadView，这样在访问某条记录时，只需要按照下边的步骤判断记录的某个版本是否可见。

- 被访问版本的trx_id **==** ReadView中的`creator_trx_id`，意味着当前事务在访问它自己修改过的记录，所以该版本可以访问
- 被访问版本的trx_id **<** ReadView中的`up_limit_id`值，表明生成该版本的事务在当前事务生成ReadView前已经提交，所以该版本可以访问
- 被访问版本的trx_id **>=** ReadView中的`low_limit_id`值，表明生成该版本的事务在当前事务生成ReadView后才开启，所以该版本不可被访问。
- 被访问版本的trx_id **between** ReadView的`[up_limit_id,low_limit_id]`，那就需要判断一下trx_id属性值是不是在 trx_ids 列表中。
	- 如果在，说明创建ReadView时生成该版本的事务还是活跃的，该版本不可以被访问。
	- 如果不在，说明创建ReadView时生成该版本的事务已经被提交，该版本可以被访问。

### 流程

1. 首先获取事务自己的版本号，也就是事务 ID； 
2. 获取 ReadView； 
3. 查询**版本链**的数据，然后与 ReadView 中的事务**版本号**进行比较；
4. 如果不符合 ReadView 规则，就需要从 Undo Log 中获取历史快照（顺着回滚指针往下走）；
5. 最后返回符合规则的数据。



### 已提交读和可重复读的区别就在于它们⽣成ReadView的策略不同

- **已提交读隔离级别**下的事务在**每次**查询的开始**都会⽣成**⼀个独⽴的ReadView，

- **可重复读隔离级别**则在**第⼀次读**的时候**⽣成⼀个ReadView**，之后的读都`复⽤`之前的ReadView。 `这样避免了不可重复读和幻读的问题`

这就是Mysql的MVCC，通过版本链，实现多版本，可并发读-写，写-读。

通过ReadView⽣成策略的不同实现不同的隔离级别。 



# 数据库的设计规范

- 第一范式：要求任何一张表必须**有主键**，每一个字段**原子性不可再分**。

- 第二范式：建立在第一范式的基础之上，要求所有非主键字段完全依赖主键，**不要产生部分依赖**。

- 第三范式：建立在第二范式的基础之上，要求所有非主键字段直接依赖主键，**不要产生传递依赖**。

<br>

## 一、第一范式

**必须有主键，并且每一个字段都是原子性不可再分**。

```
学生编号    学生姓名      联系方式
------------------------------------------
1001        张三        zs@gmail.com,1359999999
1002        李四        ls@gmail.com,13699999999
1001        王五        ww@163.net,13488888888
```

不满足第一范式，第一：没有主键。第二：联系方式可以分为邮箱地址和电话

```
学生编号(pk)    学生姓名         邮箱地址         联系电话
----------------------------------------------------------
1001            张三        zs@gmail.com    1359999999
1002            李四        ls@gmail.com    13699999999
1003            王五        ww@163.net      13488888888
```

<br>

## 二、第二范式

建立在第一范式的基础之上，要求所有**非主键字段必须完全依赖主键，不要产生部分依赖**。

```
学生编号 学生姓名 教师编号 教师姓名
-------------------------------
1001     张三     001    王老师
1002     李四     002    赵老师
1003     王五     001    王老师
1001     张三     002    赵老师
```

这张表描述了学生和老师的关系：（1个学生可能有多个老师，1个老师有多个学生）

这是非常典型的：**多对多关系！**

无主键，不满足第一范式，修改

```
学生编号+教师编号(pk)   学生姓名   教师姓名
---------------------------------------
1001      001         张三      王老师
1002      002         李四      赵老师
1003      001         王五      王老师
1001      002         张三      赵老师
```

学生编号 教师编号，两个字段联合做主键，复合主键（PK: 学生编号+教师编号）

经过修改之后，以上的表满足了第一范式。但是满足第二范式吗？

不满足，“张三”依赖1001，“王老师”依赖001，显然产生了部分依赖。

**产生部分依赖有什么缺点？**

- 数据冗余了，空间浪费了。“张三”重复了，“王老师”重复了。

<br>

使用三张表来表示多对多的关系！！！！

多对多设计：**多对多，三张表，关系表两个外键**

```
学生表
学生编号(pk)    学生名字
-----------------------
1001            张三
1002            李四
1003            王五

教师表
教师编号(pk)    教师姓名
-----------------------
001            王老师
002            赵老师

学生教师关系表
id(pk)      学生编号(fk)     教师编号(fk)
----------------------------------------
1             1001             001
2             1002             002
3             1003             001
4             1001             002
```

<br>

## 三、第三范式

第三范式建立在第二范式的基础之上

要求所有**非主键字段必须直接依赖主键，不要产生传递依赖**。

```
学生编号（PK） 学生姓名 班级编号  班级名称
--------------------------------------
1001          张三     01     一年一班
1002          李四     02     一年二班
1003          王五     03     一年三班
1004          赵六     03     一年三班
```

> 一对多，一个教室有多个学生
>
> - 是否满足第一范式？满足，有主键
>
> - 是否满足第二范式？满足，主键不是复合主键，没有产生部分依赖
>
> - 是否满足第三范式？
>
> 	- 第三范式要求：不要产生传递依赖！
>
> 		一年一班依赖01，01依赖1001，产生了传递依赖。
>
> 		不符合第三范式的要求。产生了数据的冗余。



一对多设计：**一对多，两张表，多的表加外键**

```
班级表：一
班级编号(pk)     班级名称
-------------------------
01              一年一班
02              一年二班
03              一年三班


学生表：多
学生编号（PK）  学生姓名    班级编号(fk)
-------------------------------------
1001           张三         01            
1002           李四         02            
1003           王五         03            
1004           赵六         03 
```

<br>

### 口诀

- 一对一，外键唯一
- 多对多，三张表，关系表两个外键
- 一对多，两张表，多的表加外键

<br>

## 四、巴斯-科德范式

在第三范式的改进

若一个关系达到了第三范式，并且只有一个候选键，或者每个候选键都是单属性，则该关系自然达到 BC 范式

## 五、第四范式



## 六、第五范式



# 约束

非空约束：not null

唯一性约束: unique

主键约束: primary key （简称PK）

外键约束：foreign key（简称FK） 

检查约束：check（mysql不支持，oracle支持）

## 一、为什么自增主键不连续？

- **事务回滚**（**自增值不能回退**，因为并发插入数据时，回退自增ID可能造成主键冲突）
- **唯一键冲突**（由于表的**自增值已变**，但是主键发生冲突没插进去，下一次插入主键 = 现在变了的自增值+1，所以不连续）

eg：

假设，表t里面已经有了(1,1,1)这条记录，这时我再执行一条插入数据命令：

```mysql
insert into t values(null, 1, 1); (自增id,唯一键c,普通字段d)
```

执行流程：

1. 执行器调用InnoDB引擎接口写入一行，传入的这一行的值是(0,1,1);
2. InnoDB发现用户没有指定自增id的值，获取表t当前的自增值2；
3. 将传入的行的值改成(2,1,1)；
4. 将表的自增值改成3；
5. 继续执行插入数据操作，由于已经存在c=1的记录，所以报Duplicate key error，语句返回。

这个表的自增值改成3，是在真正执行插入数据的操作之前。这个语句真正执行的时候，因为碰到唯一键c冲突，所以id=2这一行并没有插入成功，但也没有将自增值再改回去。

所以，在这之后，再插入新的数据行时，拿到的自增id就是3。也就是说，出现了自增主键不连续的情况。



## 二、主键设计

### 1、雪花算法 - 64位

#### 背景

需要选择合适的方案去应对数据规模的增长，以应对逐渐增长的访问压力和数据量。

数据库的扩展方式主要包括:业务分库、主从复制，数据库分表。

> 毫秒数在高位,自增序列在低位,整个ID都是趋势递增的。

#### 数据库分表

垂直分表

水平分表

#### 核心思想

<img src="https://cdn.jsdelivr.net/gh/YiENx1205/cloudimgs/notes/202205062237939.png" alt="image-20220506223726597" style="zoom:50%;" />

一共64位，**long 型**

首先是一个符号位，1bit标识，由于long基本类型在Java中是带符号的，最高位是符号位，正数是0，负 数是1，所以id一般是正数，最高位是0。

41bit时间截(毫秒级)，存储的是时间截的差值(当前时间截 - 开始时间截)，结果约等于69.73年。 

10bit作为机器的ID(5个bit是数据中心，5个bit的机器ID，可以部署在1024个节点)。 

12bit作为毫秒内的流水号(意味着每个节点在每毫秒可以产生 4096 个 ID)。

### 2、UUID - 36位

<img src="https://cdn.jsdelivr.net/gh/YiENx1205/cloudimgs/notes/202205062238676.png" alt="image-20220506223832810" style="zoom:50%;" />

UUID = 时间+UUID版本(16字节)- 时钟序列(4字节) - MAC地址(12字节)

其中还有四位多余的 - ，所以一共36字节

#### 为什么UUID是随机无序的呢?

因为UUID的设计中，将时间低位放在最前面，而这部分的数据是一直在变化的，并且是无序。



<br>

# 索引

**索引的本质:** `索引是数据结构`。排好序的数据结构，可以帮助快速查找数据，缩小扫描范围。

> 对于表来讲，**只要是主键或者加有unique约束的字段上会自动创建索引**

<br>

### 优点

1. 提高检索效率，**降低数据库的IO传输成本**
	- 降低IO成本是创建索引的主要目标，下面都是附带的
	- 索引是存储在外部磁盘中的
2. 通过创建唯一索引，保证表中**每行数据的唯一性**
3. **加速表和表之间的连接**，对于有依赖关系的子表和父表联合查询时，可以提高查询速度
4. 在group分组和order排序进行数据查询时，可以**减少查询中分组和排序的时间**

<br>

### 考虑添加索引的条件

- 条件1：**数据量庞大**（到底有多么庞大算庞大，这个需要测试，因为每一个硬件环境不同）

- 条件2：该**字段经常出现在where的后面**，以条件的形式存在，也就是说这个字段总是被扫描。

- 条件3：该**字段很少的DML**(insert delete update)操作。（因为DML之后，索引需要重新排序。）

详细见设计原则

<br>

### 缺点（建议不要随意添加索引）

- 因为索引也是需要动态维护的，太多的话反而会降低系统的性能，降低更新表的速度
- 而且索引会占据一定的物理空间，存储在磁盘上。

> **百万级数据量的删除（插入）**：索引可以提高查询的速度，但是会影响插入记录的速度。这种情况下，最好的办法是**先删除**表中的索引，然**后删除（插入）**数据，插入完成后**再创建索引**。

<br>

## 一、常见的索引概念

按照物理实现形式，分为**聚簇索引**和**非聚簇索引**（又称辅助索引、二级索引）

> 聚簇：表示数据行和相邻的键值聚簇的存储在一起

<br>

数据库管理存储空间的基本单位是`页`，数据库IO操作的最小单位是`页`，一页`16kb`

<br>

记录的格式

<img src="https://cdn.jsdelivr.net/gh/YiENx1205/cloudimgs/notes/202203271340960.png" width=300px />



每页的格式

<img src="https://cdn.jsdelivr.net/gh/YiENx1205/cloudimgs/notes/202203271340027.png" alt="image-20220327134057424" width=400px />



以下以InnoDB为例

### 1、聚簇索引

橙色：key

<img src="https://cdn.jsdelivr.net/gh/YiENx1205/cloudimgs/notes/202203271345391.png" alt="image-20220327131631604" />

<br>

#### 特点

- **叶子结点**存储的是完整的用户记录
- **页内**的记录是个**单向链表**
- 存放**用户记录的页**以及存放**目录项记录的页**，页和页之间组成双向链表

聚簇索引会被InnoDB默认创建，默认**自增主键**

#### 优点

- **数据访问快**，索引和数据在一个B+树中
- 对于主键的**排序查找**和**范围查找**速度快

#### 缺点

- 插入速度严重依赖于插入顺序，因为是排序的，所以会导致重新排序
- 更新主键的代价高

<br>

### 2、非聚簇索引

叶子结点不存储数据，存储的是数据行地址（MyISAM）或者主键（InnoDB），需要进行二次查找。

以c2为索引，即以c2列的大小作为排序规则：

<img src="https://cdn.jsdelivr.net/gh/YiENx1205/cloudimgs/notes/202203271358007.png" alt="image-20220327135830613" style="zoom:67%;" />

> 真实中二级索引的目录项中也会保存主键值，避免目录项值相同的情况

#### 回表：

- 通过索引找到主键，再根据主键id去主键索引查。

- 一个聚簇，多个非聚簇

#### 索引下推：

- 如果存在**被索引的列的判断条件**时，MySQL服务器将这一部分判断条件传递给存储引擎，然后由存储引擎通过**判断索引是否符合**MySQL服务器传递的**条件**，只有当索引符合条件时才会将数据检索出来返回给MySQL服务器。
- 在根据索引查询过程中就根据查询条件过滤掉一些记录，减少最后的回表操作
- 索引下推只适用于二级索引
- 比如：联合索引（name, age）

```sql
select * from tuser where name like '张%' and age=10;
```

会最左侧匹配原则，查找张，然后如果匹配多个出来，会回表，然后根据查询的结果再按照age=10过滤，但是有了索引下推后，在二级索引阶段就直接根据同为索引的age，去掉不符合的记录。

<br>

#### 非聚簇索引一定会回表吗

- **覆盖索引**：索引字段覆盖了查询语句涉及的字段，直接通过索引文件就可以返回查询所需的数据，不必通过回表操作
- **总结**：覆盖索引就不走回表

<br>

### 3、联合索引

c2和c3联合组成索引，也是非聚簇索引

**建立原则**：将查询要求频繁或者字段选择性高的列放在前面

<br>

### 4、InnoDB的B+树索引的注意事项

- **根页面位置万年不动**
	- 开始又一个数据页，保存在内存中，一旦需要创建目录页时，会将原数据页向下复制到子节点，然后将数据页改成目录页，保证根的地址不动
- 内节点中**目录项记录**的唯一性（不是索引唯一）
	- 如果表中数据c2全是1，以c2创建二级索引，查找目录页的时候不知道该往哪页找，会导致查询也很慢
	- 所以真实中**二级索引的目录项中也会保存了主键值**
- 一个页面最少存储2条记录

<br>

### 5、MyISAM中B+树索引

**叶子结点存储的是数据记录的地址**

可以理解为MyISAM中都是非聚簇索引，没有聚簇索引，需要回表

<img src="https://cdn.jsdelivr.net/gh/YiENx1205/cloudimgs/notes/202203271420200.png" alt="image-20220327141929946" />

<br>

## 二、索引的数据结构

### 1、Hash索引

- **等值查询很快**，但是范围查询的时候hash就不行了
- **存储无序**，order by的时候还需要重新排序

- 联合索引时，hash是**将联合的字段一起进行哈希计算**，可能出现别的字段联合的哈希值相同
- 索引的重复值较多时，需要遍历，效率低

Memory支持（默认），InnoDB和MyISAM不支持

<br>

但是InnoDB支持**自适应的哈希索引**：一个数据多次被访问时，会将地址存放到Hash表中

<img src="https://cdn.jsdelivr.net/gh/YiENx1205/cloudimgs/notes/202203271434665.png" alt="image-20220327143408742" style="zoom:33%;" />

<br>

### 2、B+树索引

二叉搜索树 

AVL树（平衡二叉树）

B-Tree（B：Balance，多路平衡查找树） 

B+Tree



#### 2.1、B-Tree

- 所有节点都存放数据

<img src="https://cdn.jsdelivr.net/gh/YiENx1205/cloudimgs/notes/202203271448896.png" alt="image-20220327144822430"  />

<br>

#### 2.2、B+Tree

形式见前面InnoDB索引的图

- **所有数据都保存在叶子结点**
- 从根节点到每个叶子结点的高度差不超过1，而且**同层级的节点间有指针相互链接**
- 搜索速度稳定
- 基于索引的顺序扫描时，可以利用双指针快速左右移动，效率高

> B-Tree和B+Tree各有各的使用场景，并不是B+Tree就是比B-Tree好
>
> 1. b+树的中间节点不保存数据，所以磁盘页能容纳更多节点元素，更“矮胖”。
> 2. b+树查询必须查找到叶子节点，b树只要匹配到即可不用管元素位置，因此b+树查找更稳定（并不慢）。
> 3. 对于**范围查找**来说，b+树只需遍历叶子节点链表即可，b树却需要重复地中序遍历。

<br>

## 思考题

### 1、为了减少IO，索引树会一次性加载吗

- 数据库索引时存储在磁盘上的，如果数据量很大，索引的大小也会很大
- 所以**逐一加载每一个磁盘页**，因为磁盘页对应着索引树的节点

### 2、为什么说一般查找行记录，最多只需要1～3次磁盘IO

举例：InnoDB存储引擎中**页的大小为16KB**，一般表的主键类型为INT（占用4个字节）或BIGINT（占用8个字节），指针类型也一般为4或8个字节，也就是说一个页（B+Tree 中的一个节点）中大概存储16KB/(8B+8B)=1K个键值（因为是估值，为方便计算，这里的K取值为 10^3。

也就是说一个深度为3的B+Tree 索引可以维护 10^3 * 10^3* 10^3=10 亿条记录。(这里假定一个数据页也存储10^3条行记录数据了)

实际中，每个节点不可能填充满，因此数据库中，**B+Tree的高度一般在2～4层**，InnoDB的根节点常驻内存，所以只需要1～3次磁盘IO操作。

### 3、Hash索引和B+Tree索引的区别

- Hash`不能进行范围查找`，是等值查询，因为其指向的数据是无序的（经过哈希计算后会无序），而B+的叶子结点是有序的
- Hash`不支持联合索引的最左侧原则`，即不能使用联合索引的部分索引，hash计算哈希值时将索引键合并计算，然后查找；B+Tree可以先找c2再找c3
- Hash`不支持order by排序`，也`不能模糊查询`
- `InnoDB不支持哈希索引`

<br>

## 三、索引的分类

- **功能逻辑**：普通索引、唯一性索引、主键索引、全文索引、空间索引
- **物理实现方式**：聚簇索引、非聚簇索引
- **作用字段个数**：单列索引、联合索引

<br>

1. 普通索引
	- 可以创建在任何数据类型上
2. 唯一性索引
	- 添加unique约束的字段会自动添加唯一性索引，限制该索引的值必须唯一，但可以为null
3. 主键索引
	- 主键约束，最多只有一个（索引的物理实现方式决定，数据存储在文件中只能按照一种顺序进行存储）
	- 推荐自增主键，减少页分裂
4. 单列索引
	- 建立在一个字段
5. 多列（组合、联合）索引
	- 建立在多个字段上
	- 需要遵循`最左前缀集合`（最左侧原则）
6. 全文索引
	- FULLTEXT设置，只能在char、varchar、text上
	- 搜索引擎常用，分词技术
	- 被solr、ElasticSearch等专门等搜索引擎替代
7. 空间索引
	- spatial修饰，部分引擎具有
	- 作用在空间数据类型上geometry、point、linestring、polygon

> 最左前缀原则：一个SQL利用索引时，一定要提供该索引所对应的字段中最左边的字段。
>
> 比如，针对(a, b, c)三个字段建立一个联合索引，那么在写一个sql时一定要提供a字段的条件，因为B+树索引时按照从左往右的顺序进行排序的。

<br>

## 四、索引的创建

[创建](./索引的创建.md)

创建普通索引`CREATE INDEX indexName ON table_name (column_name)`

删除普通索引`DROP INDEX [indexName] ON mytable;`

创建唯一索引`CREATE UNIQUE INDEX indexName ON mytable(username(length))`

显示索引信息`SHOW INDEX FROM table_name\G` 备注： \G 来格式化输出信息


<br>

## 五、索引的设计原则

单表不要超过6个

> - 查询快，占用空间小
> - where，匹配度高
> - 基数大，区分度高
> - 尽量扩展索引，不新建，用联合索引
> - 更新频繁不适合
> - 短索引

<br>

### 1、适合创建

1. 字段值具有唯一性的限制

> 业务上具有唯一性的字段，即使是组合字段，也必须建成唯一索引。（Alibaba）
>
> 虽然唯一性索引影响了insert的速度，这个损耗可以忽略，但是查找速度的提高是明显的。

2. **频繁作为where查询条件的字段**

	- 普通索引就可以大幅提升数据查询的效率

3. 经常group by 和order by的列

	- 索引就是让数据按照某种顺序进行存储或检索，因此当我们使用 GROUP BY 对数据进行分组查询，或者使用 ORDER BY 对数据进行排序的时候，就需要`对分组或者排序的字段进行索引`。如果待排序的列有多 个，那么可以在这些列上建立`组合索引`。

4. update、delete的where条件列

	- 如果进行更新的时候，更新的字段是非索引字段，提升的效率会更明显，这是因为非索引字段更 新不需要对索引进行维护。

5. distinct（去重）字段需要创建索引

6. 多表join连接操作时，创建索引注意事项

	- 连接表的数量尽量不超过3张，每增加一张表就相当于增加了一次嵌套的循环
	- 对where条件创建索引
	- 对用于连接的字段创建索引，并且该字段在多张表中的类型必须一致

7. 使用列的类型小的创建索引

	- 越小，占用空间越小

8. **使用字符串前缀创建索引**

	- 使用字段前几个字符作为索引

	- 区分度`count(distinct left(列名, 索引长度)) / count(*)`
	- 越接近1，重复度越小；一般33%就算比较高效了
	- 倒序存储，再前缀索引，避免前缀区分度不高的情况

9. 区分度高（散列性高）的列适合作为索引

10. 使用频繁的列放到联合索引的左侧

	- (a,b,c)，先a排序，再b再c

11. 再多个字段都要创建索引的情况下，联合索引优于单值索引



<br>

### 2、不适合创建

1. 数据量小的表最好不要使用索引（不超过100行）
2. where，group by 和 order by 中使用不到的字段，不要设置索引
3. 有大量重复数据的列上不要建立索引（高于10%），即区分度低
4. 避免对经常更新的表创建过多的索引
5. 不用无序的值作为索引
6. 删除不再使用或者很少使用的索引
7. 不要定义冗余或重复的索引
	- 冗余：定义联合索引（a,b,c），又定义索引a





## 六、索引失效的情况

1. 以“%”开头的like语句，索引无效，后缀“%”不影响

2. `or`语句前后没有`同时使用索引`

3. 列类型是字符串，一定要在条件中将数据用`引号引用`，否则失效（隐式转换）

4. 如果mysql估计使用全表扫描比索引快，则不用索引（键值少，重复数据多）

5. 组合索引要遵守`最左前缀原则`——不使用第一列，索引失效

6. 在索引字段上使用not，<>，!= （对它处理是全表扫描）

7. **计算**、**函数**、类型转换（自动或手动）导致索引失效

	```sql
	# 失效
	select * fromuserwhere age-1 =10;
	```

	

8. is null可以使用索引，is not null无法使用索引

9. 范围条件右边的列索引失效

10. 数据库和表的字符集统一使用utf8mb4



# 锁

<img src="https://cdn.jsdelivr.net/gh/YiENx1205/cloudimgs/notes/202205112056261.png" alt="image-20220511205635387" style="zoom: 50%;" />

## 从类型划分：读锁S、写锁X

### 锁定读

- 对读取的记录加S锁

```sql
select ... lock in share mode;
# or
select ... for share; #8.0新增语法
```

- 对读取的记录加X锁

```sql
select ... for update;
```

查询语句不会产生锁，但是select ..... for update例外

- 如果查询条件**用了索引**/主键，那么就会进行`行锁`。

- 如果是普通字段(**没有索引**/主键)，那么就会进行`表锁`。

### 写操作

- `DELETE`：对一条记录做DELETE操作的过程其实是先在`B+`树中定位到这条记录的位置，然后获取这条记录的`X锁`，再执行`delete mark`操作。

- `UPDATE`：在对一条记录做UPDATE操作时分为三种情况：

	- 情况1：未修改该记录的键值，并且被更新的列占用的存储空间在修改前后未发生变化。

		则先在`B+`树中定位到这条记录的位置，然后再获取一下记录的`X锁`，最后在原记录的位置进行修改操作。

	- 情况2：未修改该记录的键值，并且至少有一个被更新的列占用的存储空间在修改前后发生变化。

		则先在`B+`树中定位到这条记录的位置，然后获取一下记录的`X锁`，将该记录彻底删除掉（就是把记录彻底移入垃圾链表），最后再插入一条新记录。新插入的记录由`INSERT`操作提供的`隐式锁`进行保护。

	- 情况3：修改该记录的键值，则相当于在原记录上做`DELECT`操作之后再来一次`INSERT`操作。

- `INSERT`：一般情况下，新插入一条记录的操作并不加锁，通过一种称之为`隐式锁`的结构来保护这条新插入的记录在本事务提交前不被别的事务访问。

## 锁粒度划分：表锁、行锁、页锁

- 表级锁：开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高，并发度最低。
- 行级锁：开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高。
- 页面锁：开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般

### 表锁

#### 1、表级别的S锁、X锁

一般情况下，不会使用InnoDB存储引擎提供的表级别的`S锁`和`X锁`。只会在一些特殊情况下，比方说`崩溃恢复`过程中用到。比如，在系统变量`autocommit=0，innodb_table_locks = 1`时，`手动`获取InnoDB存储引擎提供的表t 的`S锁`或者`X锁`可以这么写：

- `LOCK TABLES t READ`：InnoDB存储引擎会对表`t`加表级别的`S锁`。 

- `LOCK TABLES t WRITE`：InnoDB存储引擎会对表`t`加表级别的`X锁`。

总结：MyISAM在执行查询语句（SELECT）前，会给涉及的所有表加读锁，在执行增删改操作前，会给涉及的表加写锁。`InnoDB`存储引擎是不会为这个表添加表级别的`读锁`或者`写锁`的。

#### 2、意向锁

InnoDB 支持`多粒度锁（multiple granularity locking）`，它允许`行级锁`与`表级锁`共存，而**意向锁**就是其中的一种`表锁`。

**意向锁要解决的问题**

在数据表的场景中，**如果我们给某一行数据加上了排它锁，数据库会自动给更大一级的空间，比如数据页或数据表加上意向锁，告诉其他人这个数据页或数据表已经有人上过排它锁了**，这样当其他人想要获取数据表排它锁的时候，只需要了解是否有人已经获取了这个数据表的意向排它锁即可，其他人进行操作就会自动阻塞，不需要再去找是否存在排他锁了。

- 如果事务想要获取数据表中某些记录的共享锁，就需要在数据表上`添加意向共享锁`
- 如果事务想要获取数据表中某些记录的排它锁，就需要在数据表上`添加意向排他锁`

这时，意向锁会告诉其他事务已经有人锁定了表中的某些记录。

**意向锁分为两种**：

- **意向共享锁**（intention shared lock, IS）：事务有意向对表中的某些行加**共享锁**（S锁）

```mysql
-- 事务要获取某些行的 S 锁，必须先获得表的 IS 锁。 
SELECT column FROM table ... LOCK IN SHARE MODE;
```

- **意向排他锁**（intention exclusive lock, IX）：事务有意向对表中的某些行加**排他锁**（X锁）

```mysql
-- 事务要获取某些行的 X 锁，必须先获得表的 IX 锁。 
SELECT column FROM table ... FOR UPDATE;
```

即：意向锁是由存储引擎`自己维护的`，用户无法手动操作意向锁，在为数据行加共享 / 排他锁之前，InooDB 会先获取该数据行`所在数据表的对应意向锁`。

#### 3、自增锁

`AUTO_INCREMENT`

#### 4、元数据锁

MDL 的作用是，保证读写的正确性。比如，如果一个查询正在遍历一个表中的数据，而执行期间另一个线程对这个`表结构做变更`，增加了一列，那么查询线程拿到的结果跟表结构对不上，肯定是不行的。

因此，**当对一个表做增删改查操作的时候，自动加MDL读锁；当要对表做结构变更操作的时候，自动加MDL写锁。**

### 行锁

行锁（Row Lock）也称为记录锁，顾名思义，就是锁住某一行（某条记录row）。需要注意的是，MySQL服务器层并没有实现行锁机制，**行级锁只在存储引擎层实现。**

**优点：**锁定力度小，发生`锁冲突概率低`，可以实现的`并发度高`

**缺点：**对于`锁的开销比较大`，加锁会比较慢，容易出现`死锁`情况

InnoDB与MyISAM的最大不同有两点：一是支持事务；二是采用了行级锁。

#### 1、记录锁

记录锁 Record Lock 也就是仅仅把一条记录锁上，官方的类型名称为：`LOCK_REC_NOT_GAP`。

记录锁是有S锁和X锁之分的，称之为`S型记录锁`和`X型记录锁`。

- 当一个事务获取了一条记录的S型记录锁后，其他事务也可以继续获取该记录的S型记录锁，但不可以继续获取X型记录锁；
- 当一个事务获取了一条记录的X型记录锁后，其他事务既不可以继续获取该记录的S型记录锁，也不可以继续获取X型记录锁。

#### 2、间隙锁

`MySQL`在`REPEATABLE READ`隔离级别下是可以解决幻读问题的，解决方案有两种，可以使用`MVCC`方案解决，也可以采用`加锁`方案解决。

但是在使用加锁方案解决时有个大问题，就是事务在第一次执行读取操作时，那些幻影记录尚不存在，我们无法给这些`幻影记录`加上`记录锁`。InnoDB提出了一种称之为`Gap Locks`的锁，官方的类型名称为：`LOCK_GAP`，我们可以简称为`gap锁`。

<img src="https://cdn.jsdelivr.net/gh/YiENx1205/cloudimgs/notes/202205112105930.png" alt="image-20220511210540436" style="zoom:50%;" />

图中id值为8的记录加了gap锁，意味着 不允许别的事务在id值为8的记录前边的间隙插入新记录 ，其实就是 id列的值(3, 8)这个区间的新记录是不允许立即插入的。

比如，有另外一个事务再想插入一条id值为4的新记录，它定位到该条新记录的下一条记录的id值为8，而这条记录上又有一个gap锁，所以就会阻塞插入操作，直到拥有这个gap锁的事务提交了之后，id列的值在区间(3, 8)中的新记录才可以被插入。

**gap锁的提出仅仅是为了防止插入幻影记录而提出的**。虽然有`共享gap锁`和`独占gap锁`这样的说法，但是它们起到的作用是相同的。而且如果对一条记录加了gap锁（不论是共享gap锁还是独占gap锁），并不会限制其他事务对这条记录加记录锁或者继续加gap锁。

#### 3、临键锁

Next-Key Locks = Record + Gap，锁住当前记录

既`锁住某条记录`，又`阻止`其他事务在该记录前边的`间隙插入新记录`，官方的类型名称为：`LOCK_ORDINARY`。Next-Key Locks是在存储引擎**Innodb**、事务级别在**可重复读**的情况下使用的数据库锁，innodb默认的锁就是Next-Key locks。 

```mysql
begin; 
select * from student where id <=8 and id > 3 for update;
```

#### 4、插入意向锁

一个事务在`插入`一条记录时需要判断一下插入位置是不是被别的事务加了`gap锁`（`next-key锁`也包含`gap锁`），如果有的话，插入操作需要等待，直到拥有`gap锁`的那个事务提交。但是**InnoDB规定事务在等待的时候也需要在内存中生成一个锁结构**，表明有事务想在某个`间隙`中`插入`新记录，但是现在在等待。

InnoDB就把这种类型的锁命名为`Insert Intention Locks`，官方的类型名称为：`LOCK_INSERT_INTENTION`，我们称为`插入意向锁`。

插入意向锁是一种`Gap锁`，不是意向锁，在insert操作时产生。

插入意向锁是在插入一条记录行前，由`INSERT 操作产生的一种间隙锁`。

事实上**插入意向锁并不会阻止别的事务继续获取该记录上任何类型的锁。**

## 死锁问题

https://blog.csdn.net/qq_24691007/article/details/122932768

InnoDB存储引擎采用了一种叫作**等待图**（wait-for graph）的方法来自动检测死锁，如果发现死锁，就会自动回滚一个事务。

- 尽量让数据表中的数据检索都通过索引来完成，避免无效索引导致行锁升级为表锁。
- 合理设计索引，尽量缩小锁的范围。
- 尽量减少查询条件的范围，尽量避免间隙锁或缩小间隙锁的范围。
- 尽量控制事务的大小，减少一次事务锁定的资源数量，缩短锁定资源的时间。
- 如果一条SQL语句涉及事务加锁操作，则尽量将其放在整个事务的最后执行。
- 尽可能使用低级别的事务隔离机制。



## 锁结构

<img src="https://cdn.jsdelivr.net/gh/YiENx1205/cloudimgs/notes/202205112125785.png" alt="image-20220405151409557" style="zoom:80%;" />



# 分析查询语句：EXPLAIN

查询sql语句的执行计划，查看sql语句有没有使用索引，有没有全表扫描等。

```sql
EXPLAIN SELECT select_options 
#或者
DESCRIBE SELECT select_options
```



| 列名          | 描述                                                     |
| ------------- | -------------------------------------------------------- |
| id            | 在一个大的查询语句中每个SELECT关键字都对应一个`唯一的id` |
| select_type   | SELECT关键字对应的那个查询的类型                         |
| table         | 表名                                                     |
| partitions    | 匹配的分区信息                                           |
| type          | 针对单表的访问方法                                       |
| possible_keys | 可能用到的索引                                           |
| key           | 实际上使用的索引                                         |
| key_len       | 实际使用到的索引长度                                     |
| ref           | 当使用索引列等值查询时，与索引列进行等值匹配的对象信息   |
| rows          | 预估的需要读取的记录条数                                 |
| filtered      | 某个表经过搜索条件过滤后剩余记录条数的百分比             |
| Extra         | 一些额外的信息                                           |



# 数据库调优

哪些维度可以进行数据库调优？

- 索引失效，没有充分利用到索引——索引建立
- 关联查询太多JOIN（设计缺陷或不得已的需求）——SQL优化
- 服务器调优及各个参数设置（缓冲、线程数等）——调整my.cnf
- 数据过多——分库分表

# 分库分表

## 一、垂直切分

将字段拆分为多张表，需要一定的重构。

结构、数据都不一样。

所有库的并集为全量数据；分表的话至少有一列交集，用于关联数据，所有表的并集为全量数据。

场景：用户表中既有用户登录信息又有用户基本信息，可以拆成两个单独的表。

## 二、水平切分

将数据分散到多张表，设计**分区键**

结构一样，数据不一样，没有交集

库多了可以缓解io和cpu压力；表中数据减少可以提高sql执行效率，减轻cpu压力。

场景：就是数量多就可以分。

## 分表后

### 一、分表后的ID怎么保证唯一性

数据库的id大多都是自增，当联表查询数据时就会出现主键重复的情况

1. **设定步长**，比如1-1024张表我们设定1024的基础步长，这样主键落到不同的表就不会冲突了。
2. **分布式ID**，比如雪花算法
3. **不使用主键作为查询依据**，而是每张表单独新增一个字段作为唯一主键使用，比如订单表订单号是唯一的，不管最终落在哪张表都基于订单号作为查询依据，更新也一样。

### 二、分表后的排序

#### 1、排序字段是唯一索引

1. 首先第一页的查询：将各表的结果集进行合并，再排序。
2. 第二页及以后的查询：传入上一页排序字段的最后一个值，及排序方式
3. 根据排序方式，及这个值进行查询。如排序字段date，上一页最后值为3，排序方式降序。查询的时候sql为`select ... from table where date < 3 order by date desc limit 0,10`。这样再将几个表的结果合并排序即可。

#### 2、排序字段不是唯一索引

1. 第一步不变

2. 第二步在传入上一个的基础上，还需要传入能确定该行记录的唯一性字段

3. sql需要进行修改为`select ... from table where date = 3 order by date desc union select ... from table where date < 3 order by date desc limit 0,10`。然后将结果合并之后排序。根据唯一性字段确定上一页最后一条记录，然后找出下面的分页记录。

这里为何不用`select ... from table where date <= 3 order by date desc limit 0,10`呢，因为这样只能取到10条记录，如果某台节点上的满足等于3的节点为11条，那么就会漏掉一条数据，导致查询结果不正确

## 路由规则

水平拆分之后，数据都是分在不同的库或者表里面，如何知道哪条数据在那个库或者表里，需要路由算法来进行计算。

### 1、范围路由

选择有序的数据列，作为路由的条件，根据不同分段分散到不同的数据库表中，举个例子拿用户id 来说，可以将前0-500W放在一个数据库的表里面， 500W+1 - 1000W放到另外一个数据库的表里面，一次类推。

这样就可以根据范围进行路由了。但是这种方法复杂的地方就在于这个范围的选择。

但是他的好处也就是你后续扩充的话 ，简单方便，原来的数据不用进行改动。

缺点：容易造成热点数据的访问倾斜，热点数据集中在一个表里面。

### 2、Hash算法

选取一个列或者多个列进行hash运算，然后根据hash 的结果将不同的数据分散到不同的数据库中，有点那种散列表的意思，然后后续访问的时候使用同样的hash运算，就可以找到在哪个表里面了。

hash算法的复杂点就在于初始表的数量的选取上，也就是hash算法的基数的选择。

hash算法的优缺点和范围路由的优缺点正好相反。

### 3、路由配置

路由配置就是设置一个路由表，用一张独立的表来记录陆游信息。我们将数据存放的位置存放到这个表里面。

优点是：后续改动方便，只需要改动路由表 。

缺点就是 每次必须多查询一次路由表，而且随着数据量的增大，路由表也会成为瓶颈点。

# 慢查询

超过`long_query_time`参数设定的时间阈值（默认10s），就被认为是慢的，是需要优化的。慢查询被记录在慢查询日志里。慢查询日志默认是不开启的。如果需要优化SQL语句，就可以开启这个功能，它可以让你很容易地知道哪些语句是需要优化的。

```shell
查询是否开启慢查询日志: show variables like 'slow_query_log';
开启慢查询sql: set global slow_query_log = 1/on;
关闭慢查询sql: set global slow_query_log = 0/off;

询未使用索引是否开启记录慢查询日志:
show variables like 'log_queries_not_using_indexes';
开启记录未使用索引sql：set global log_queries_not_using_indexes=1/on
关闭记录未使用索引sql：set global log_queries_not_using_indexes=1/off

查询超过多少秒的记录到慢查询日志中:
show variables like 'long_query_time';
设置超1秒就记录慢查询sql：set global long_query_time= 1;
```

## 一、慢查询的配置文件

MySQL的配置文件my.cnf

```shell
long_query_time = 10
log-slow-queries = /var/lib/mysql/mysql-slow.log
```

`long_query_time`是指执行超过多久的SQL会被日志记录下来，这里是10 秒。

`log-slow-queries`设置把日志写在哪里。为空的时候，系统会给慢查询日志赋予主机名，并加上slow.log。如果设置了参数`log-long-format`，那么所有没有使用索引的查询也将被记录。

## 二、慢查询解读

第一行：记录时间

第二行：用户名 、用户的IP信息、线程ID号

第三行：执行花费的时间【单位：毫秒】、执行获得锁的时间、获得的结果行数、扫描的数据行数

第四行：这SQL执行的时间戳

第五行：具体的SQL语句

## 三、慢查询优化

1. 检查是否使用了索引
2. 检查使用的索引是否最优
3. 检查所查字段是否都是必须的，是否查询了过多字段，查出了多余数据
4. 检查表中数据是否过多，是否应该分库分表
5. 检查数据库实例所在机器的性能配置，适当增加资源



# MySQL主从同步

## 一、原理

三个线程

实际上主从同步的原理就是基于 binlog 进行数据同步的。在主从复制过程中，会基于3个线程来操作，一**个主库线程(Binlog dump thread)**，**两个从库线程(I/O thread、SQL thread)**。

> binlog 是记录所有数据库**表结构变更**（例如CREATE、ALTER TABLE…）以及表**数据修改**（INSERT、UPDATE、DELETE…）的二进制日志。
>
> binlog 是server层的，所有引擎都可以使用，用于主从复制和数据恢复
>
> Note：redo log 是引擎层面的，只有 Innodb 才有，用于崩溃恢复

<img src="https://cdn.jsdelivr.net/gh/YiENx1205/cloudimgs/notes/202207091059187.png" alt="image-20220709105755994" style="zoom:50%;" />

**二进制日志转储线程** (Binlog dump thread)是一个主库线程。当从库线程连接的时候， 主库可以将二进制日志发送给从库，当主库读取事件(Event)的时候，会在 Binlog 上**加锁**，读取完成之后，再将锁释放掉。

**从库 I/O 线程**会连接到主库，向主库发送请求更新Binlog。这时从库的I/O线程就可以读取到主库的二进制日志转储线程发送的 Binlog 更新部分，并且拷贝到本地的**中继日志** (Relay log)。

**从库 SQL 线程**会读取从库中的中继日志，并且执行日志中的事件，将从库中的数据与主库保持同步。

### 1、步骤

- Master 将写操作记录到 binlog
- Slave 将 Master 的 binary log events 拷贝到 relay log
- Slave 重做 relay log 中的 events，将改变应用到自己的数据库中。 

> MySQL复制是异步的且串行化的，而且重启后从**接入点**开始复制。



## 二、数据同步一致性问题

mysql默认是异步的复制方式，主库把日志发给从库后不关心从库是否已经处理。假设主库挂掉，从库处理失败，此时从库升级为主库后，日志就丢失了。

### 1、全同步复制

主库写入binlog后强制同步日志到从库，所有从库都执行完之后才返回给客户端。性能影响较大。不会导致一致性问题。

### 2、半同步复制

从库写入日志成功后返回ACK确认给主库，主库收到至少一个从库的确认就认为写操作成功。会导致一致性问题。

等待的从库数有参数可以调节。

<img src="https://cdn.jsdelivr.net/gh/YiENx1205/cloudimgs/notes/202207091605956.png" alt="iShot_2022-07-09_16.04.06" style="zoom:70%;" />

### 3、组复制

简称 MGR (MySQL Group Replication)。这种复制技术是基于 Paxos 协议的状态机复制。

将多个节点共同组成一个**复制组**，在**执行读写(RW)事务**的时候，需要通过一致性协议层 (Consensus 层)的同意，也就是读写事务想要进行提交，必须要经过**组里“大多数人”(对应 Node 节点)的同意**，大多数指的是同意的节点数量需要**大于 (N/2+1)**，这样才可以进行提交，而不是原发起方一个说了算。而针对**只读(RO)事务**则不需要经过组内同意，直接 COMMIT 即可。

<img src="https://cdn.jsdelivr.net/gh/YiENx1205/cloudimgs/notes/202207091623978.png" alt="iShot_2022-07-09_16.22.53" style="zoom:50%;" />

# 书写SQL的建议

[书写SQL的建议](https://mp.weixin.qq.com/s?__biz=Mzg2OTA0Njk0OA==&mid=2247486461&idx=1&sn=60a22279196d084cc398936fe3b37772&chksm=cea24436f9d5cd20a4fa0e907590f3e700d7378b3f608d7b33bb52cfb96f503b7ccb65a1deed&token=1987003517&lang=zh_CN%23rd)

#### 1、查询SQL尽量不要使用select *，而是select具体字段。

- 只取需要的字段，节省资源、减少网络开销。
- select * 进行查询时，很可能就不会使用到覆盖索引了，就会造成回表查询。

#### 2、如果知道查询结果只有一条或者只要最大/最小一条记录，建议用limit 1

- 加上limit 1后,只要找到了对应的一条记录,就不会继续向下扫描了,效率将会大大提高。
- 当然，如果name是唯一索引的话，是不必要加上limit 1了，因为limit的存在主要就是为了防止全表扫描，从而提高性能,如果一个语句本身可以预知不用全表扫描，有没有limit ，性能的差别并不大。

#### 3、应尽量避免在where子句中使用or来连接条件

```sql
# 反例
select * from user where userid=1 or age=18
# 正例

# 使用union all
select * from user where userid=1
unionall
select * from user where age = 18

# 或者分开两条sql写：
select * from user where userid=1
select * from user where age = 18
```

- 使用or可能会使索引失效，从而全表扫描。

> 对于or + 没有索引的age这种情况，假设它走了userId的索引，但是走到age查询条件时，它还得全表扫描，也就是需要三步过程：全表扫描+索引扫描+合并 如果它一开始就走全表扫描，直接一遍扫描就完事。mysql是有优化器的，处于效率与成本考虑，遇到or条件，索引可能失效，看起来也合情合理。

#### 4、优化limit分页

偏移量过大的时候，效率会变慢

反例：

```sql
selectid，name，age from employee limit 10000，10
```

- 因为Mysql并非是跳过偏移量直接去取后面的数据，而是先把偏移量+要取的条数读出来，然后再把前面偏移量这一段的数据抛弃掉再返回的。

解决

```sql
# 方案一 ：返回上次查询的最大记录(偏移量)
select id, name from employee where id > 10000 limit 10
# 只适用于自增主键

# 方案二：orderby + 索引
select id, name from employee order by id limit 10000, 10

# 方案三：在业务允许的情况下限制页数：大多数用户都不会往后翻太多页

# 方案四：limit id
select id, name from employee 
where id>(select id from employee limit 2000,1) limit 20
# 由于id是主键，拥有主键索引，所以对一个主键id进行limit范围查询，
# 相比于select * from areas limit 2000,20;速度会快很多。


```



#### 5、避免索引失效的情况

like模糊查询的时候，%在最前面可能会造成索引失效



#### 6、使用where条件限定要查询的数据，避免返回多余的行

```java
Long userId = sqlMap.queryObject(
    "select userId from user where userId='userId'and isVip='1'"
)
boolean isVip = userId！=null;
```



#### 7、Inner join 、left join、right join，优先使用Inner join

> - Inner join 内连接，在两张表进行连接查询时，只保留两张表中完全匹配的结果集
> - left join 在两张表进行连接查询时，会返回左表所有的行，即使在右表中没有匹配的记录。
> - right join 在两张表进行连接查询时，会返回右表所有的行，即使在左表中没有匹配的记录。

如果要使用left join，左边表数据结果尽量小，**如果有条件的尽量放到左边处理**。

反例

```sql
select * from tab1 t1 left join tab2 t2 on t1.size = t2.size where t1.id>2;
```

解决

```sql
select * from (select * from tab1 where id >2) t1 left join tab2 t2 on t1.size = t2.size;
```



#### 8、如果插入数据过多，考虑批量插入

```java
for(User u :list){
 INSERT into user(name,age) values(#name#,#age#)   
}

// 批量插入性能好，节省时间
//一次500批量插入，分批进行
insert into user(name,age) values
<foreach collection="list" item="item" index="index" separator=",">
    (#{item.name},#{item.age})
</foreach>
```

